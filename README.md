ğŸ“œ README.md (for your GitHub repository)

# ğŸ—‘ï¸ Automated Waste Sorting using Computer Vision & YOLOv8

![Project Banner](https://via.placeholder.com/1200x400?text=Automated+Waste+Sorting)  
*A deep learning-based approach for real-time waste detection and classification using YOLOv8 and MongoDB.*

## ğŸš€ Overview
This project utilizes **YOLOv8** to detect and classify different types of litter in real-time using **computer vision**. The trained model helps in automated waste sorting, making recycling more efficient.

## ğŸ“‚ Dataset
We use the **TACO (Trash Annotations in Context)** dataset, which consists of labeled images of waste items for training our object detection model.

- **Dataset Source:** [Roboflow TACO Dataset](https://universe.roboflow.com/mohamed-traore-2ekkp/taco-trash-annotations-in-context)
- **Classes:** 60 different types of waste materials (plastic, metal, paper, etc.)

## ğŸ› ï¸ Installation
To set up the project, follow these steps:

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/ziatily2/Automated-Waste-Sorting-using-Computer-Vision-and-MongoDB.git
cd Automated-Waste-Sorting-using-Computer-Vision-and-MongoDB

2ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

If you donâ€™t have requirements.txt, install them manually:

pip install ultralytics opencv-python roboflow numpy torch torchvision matplotlib

3ï¸âƒ£ Download the Dataset

Run the following in a Jupyter Notebook or Google Colab:

from roboflow import Roboflow
rf = Roboflow(api_key="YOUR_API_KEY")
project = rf.workspace("mohamed-traore-2ekkp").project("taco-trash-annotations-in-context")
version = project.version(16)
dataset = version.download("yolov8")

This will download the dataset into the current directory.
ğŸ¯ Training the Model

To train the YOLOv8 model on the dataset:

from ultralytics import YOLO

# Load YOLOv8 model
model = YOLO("yolov8s.pt")

# Train the model
results = model.train(data="tacoily.yaml", epochs=100, imgsz=640, batch=16)

    epochs=100: Train for 100 epochs.
    imgsz=640: Image size set to 640x640.
    batch=16: Batch size of 16.

The trained weights will be saved in:

runs/detect/train/weights/best.pt

ğŸ¥ Real-Time Detection

Run the trained model to detect waste in real-time using a webcam.

import cv2
from ultralytics import YOLO

# Load trained model
model = YOLO("runs/detect/train/weights/best.pt")

# Initialize webcam
cap = cv2.VideoCapture(0)

while cap.isOpened():
    success, frame = cap.read()
    if not success:
        print("Failed to capture frame")
        break
    
    # Perform inference
    results = model(frame)

    # Draw detections on the frame
    annotated_frame = results[0].plot()

    # Show the frame
    cv2.imshow("YOLOv8 Waste Detection", annotated_frame)

    # Press 'q' to exit
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

ğŸ“ˆ Model Performance

After training, evaluate the model using:

metrics = model.val()

This outputs:

    Precision, Recall, and mAP (mean Average Precision).
    Confusion matrix and detection results.

ğŸ“Œ Directory Structure

ğŸ“‚ Automated-Waste-Sorting-using-Computer-Vision-and-MongoDB
 â”œâ”€â”€ ğŸ“‚ dataset
 â”‚   â”œâ”€â”€ train/
 â”‚   â”œâ”€â”€ val/
 â”‚   â”œâ”€â”€ test/
 â”œâ”€â”€ ğŸ“‚ runs
 â”‚   â”œâ”€â”€ detect/
 â”‚   â”‚   â”œâ”€â”€ train/
 â”‚   â”‚   â”‚   â”œâ”€â”€ weights/
 â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ best.pt
 â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ last.pt
 â”œâ”€â”€ ğŸ“ README.md
 â”œâ”€â”€ ğŸ“ data.yaml
 â”œâ”€â”€ ğŸ“ tacoily.yaml
 â”œâ”€â”€ ğŸ“œ infer.py
 â”œâ”€â”€ ğŸ“œ train.py
 â”œâ”€â”€ ğŸ“œ detector.py
 â”œâ”€â”€ ğŸ“¦ requirements.txt

ğŸ› ï¸ Issues & Troubleshooting

    Model not detecting all classes correctly?
    ğŸ”¹ Ensure the dataset is properly labeled.
    ğŸ”¹ Train for more epochs (epochs=200).
    ğŸ”¹ Try different confidence thresholds (conf=0.25).

    Webcam not working?
    ğŸ”¹ Make sure OpenCV is installed (pip install opencv-python).
    ğŸ”¹ Use cv2.VideoCapture(1) if 0 does not work.
